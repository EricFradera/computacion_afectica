# Consideraciones éticas

El monitoreo constante de la computación afectiva crea muchas preguntas sobre cómo serán utilizados los datos de los usuarios. Es una incógnita de qué forma se materializa la computación afectiva en el futuro, pero sí que hoy en día ya se pueden hacer preguntas sobre cómo afectan a los usuarios la implementación de estas tecnologías en nuestro día a día.

La detección de mentiras es un campo que se ha investigado por décadas. Los polígrafos nunca han sido instrumentos precisos, pero sí que generan una realidad incómoda donde se arrebata al usuario la capacidad de mentir, ya que la máquina sabrá cuando está miente o no. Las implicaciones de un instrumento que detecta mentiras o emociones ponen en peligro la libertad de expresión al poner en peligro a pensamientos disidentes de una realidad política o económica. En una sociedad más autoritaria se podría utilizar estas tecnologías para descubrir aquellas personas que no creen en un régimen, eliminando las diferencias de pensamiento dentro de una sociedad.

Estas últimas conjeturas ya han sido usadas en países como Iraq o países del primer mundo como Estados Unidos. Aunque actualmente estos mecanismos se utilizan principalmente en prisiones o con presuntos delincuentes en juicios, sí que crea la incógnita de si estas tecnologías se utilizarán en los ciudadanos.

El control de masas ya es un aspecto altamente controvertido, pero que a través de la computación afectiva tecnologías como la detección facial, le detección de emociones vía texto se automatiza y se masifica el control social del contenido posteado online e incluso el del mundo real a través de cámaras o micrófonos.

Los dispositivos inteligentes forman parte del día a día de los usuarios y son herramientas más que capaces de detectar emociones e inferir a través de los datos obtenidos pensamientos políticos o ideológicos.

La manipulación emocional es otro tipo de inquietud que se genera derivado de la computación afectiva. Conociendo las emociones de los usuarios frente a algún tipo de contenido no es difícil de pensar cómo se pueden utilizar estas para modificar su forma de pensamiento. Durante años se ha analizado los rabbit holes de la ultraderecha que radicalizan a jóvenes poco a poco hasta llevarlos a visiones políticas extremistas normalmente a través de un discurso basado en los miedos e inquietudes de la población. El conocimiento y adaptación de contenido de los usuarios es peligrosa por la capacidad que tienen de doblegar poco a poco el sentido común.

Viendo el panorama actual podemos observar cómo algunas redes sociales como Facebook han utilizado la ira para mejorar el *engagement* de sus usuarios. Se analizó y se observó que las publicaciones que generaban ira hacia algún grupo creaban mayor interacción ya que se reacciona, se compartía y se comentaba mucho más que en publicaciones normales. Esto ha hecho que las fakeNews que tienen como objetivo de repartir información falsa, sean extremadamente populares dentro de esta plataforma. A causa de esto Facebook se ha convertido en una de las mayores fuentes de información difamatoria.

Esta manipulación emocional no tiene que venir sólo desde puntos de vista políticos sino económicos. Ya es práctica común la recomendación de anuncios en base a gustos personales de plataformas como Google ads o algoritmos de recomendación de contenido como tik tok. Hoy en día estos algoritmos ya se utilizan para comprar usuarios sólo mediante el contenido que estos buscan. Tener un input emocional genera situaciones incómodas en las que los anuncios se posicionan en momentos de debilidad donde el usuario tiene la guardia más baja y tiene mayor probabilidad de hacer clic.

Otro elemento en riesgo es la generación de contenido en base a estudios de mercado. A través del análisis de las emociones de los usuarios en contenido como películas o música existe el peligro de reducción de la variedad artística al solo invertir en base a aquellas películas que generan mayor *engagement*, reduciendo la variedad y creando grandes carencias artísticas. 

Finalmente es imprescindible tener siempre en cuenta de donde se obtiene la información y de qué forma afecta a diferentes tipos de población. La computación afectiva debería ser justa independientemente de la etnia, género, edad … Aunque en la mayoría de los casos estos sesgos no se hacen de forma intencional, sí que pueden suceder por una lista muy larga de motivos, desde los miembros que participan en equipo o proyecto o los participantes dentro de un dataset que entrena un algoritmo de inteligencia artificial.

Este pequeño análisis sobre los peligros éticos que la computación afectiva plantea algunos de los grandes peligros que podrían llegar a aparecer en una posible transición hacia esta línea de diseño. Aunque aún es pronto para saber qué dirección se tomará es de urgente un análisis riguroso de las tendencias dentro del campo para establecer un marco ético que permita a los desarrolladores crear productos innovadores al mismo tiempo que respetan a los usuarios.